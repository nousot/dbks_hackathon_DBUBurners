{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9da67c3-4c4c-4f22-b9ec-3beff07ec6bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# structured fields go here\n",
    "mining_qa_prompts = {\n",
    "  \"title\": \"Q&A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA Requiring Diverse World Knowledge\",\n",
    "  \"authors\": [\"Haibo Wang\", \"Weifeng Ge\", \"...\"],\n",
    "  \"affiliation\": \"School of Computer Science, Fudan University\",\n",
    "  \"focus\": \"Enhancing AI models for Visual Question Answering using Q&A Prompts method\",\n",
    "  \"methodology\": [\"Training a visual question generation model\", \"Generating question-answer prompts\", \"Reasoning with these prompts\"],\n",
    "  \"key_findings\": \"Improvement in AI's ability to answer complex visual questions with diverse world knowledge\",\n",
    "  \"publication_date\": \"2024-01-19\"\n",
    "}\n",
    "\n",
    "clinical_document_qa = {\n",
    "  \"title\": \"Dynamic Q&A of Clinical Documents with Large Language Models\",\n",
    "  \"authors\": [\"Ran Elgedawy\", \"Sudarshan Srinivasan\", \"Ioana Danciu\", \"...\"],\n",
    "  \"affiliation\": \"University of Tennessee Knoxville, Oak Ridge National Laboratory\",\n",
    "  \"focus\": \"Development of a conversational interface using LLMs for querying clinical notes\",\n",
    "  \"key_technologies\": [\"Large Language Models\", \"Semantic Embedding Models\"],\n",
    "  \"applications\": \"Efficient querying and retrieval in EHRs\",\n",
    "  \"challenges_future_work\": [\"Model optimization\", \"Domain-specific fine-tuning\", \"Evaluation challenges\"],\n",
    "  \"publication_date\": \"2024-01-19\"\n",
    "}\n",
    "\n",
    "medusa = {\n",
    "  \"title\": \"MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads\",\n",
    "  \"authors\": [\"Tianle Cai\", \"Yuhong Li\", \"Zhengyang Geng\", \"...\"],\n",
    "  \"affiliations\": [\"Princeton University\", \"University of Illinois Urbana-Champaign\", \"Carnegie Mellon University\", \"...\"],\n",
    "  \"focus\": \"Accelerating LLM inference with MEDUSA method\",\n",
    "  \"key_concepts\": [\"Multiple decoding heads\", \"Tree-based attention mechanism\", \"Fine-tuning strategies\"],\n",
    "  \"results\": \"Significant speedup in LLM inference with minimal quality compromise\",\n",
    "  \"publication_date\": \"2024-01-19\"\n",
    "}\n",
    "\n",
    "antisemitic_detection = {\n",
    "  \"title\": \"Detection of Emerging Coded Antisemitic Terminology in Online Posts\",\n",
    "  \"authors\": [\"Dhanush Kikkisetti\", \"Raza Ul Mustafa\", \"Wendy Melillo\", \"...\"],\n",
    "  \"affiliation\": \"American University\",\n",
    "  \"focus\": \"Detecting coded antisemitic terminology in social media posts\",\n",
    "  \"methodology\": [\"Extraction of trending terms\", \"Comparison with known terms\", \"Fine-tuning BERT model\"],\n",
    "  \"key_findings\": \"Identification of new antisemitic terms and development of methodologies for their detection\",\n",
    "  \"publication_date\": \"2024-01-19\"\n",
    "}\n",
    "\n",
    "jailbreak_resistance = {\n",
    "  \"title\": \"Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning\",\n",
    "  \"authors\": [\"Adib Hasan\", \"Ileana Rugina\", \"Alex Wang\"],\n",
    "  \"affiliation\": \"Massachusetts Institute of Technology (MIT)\",\n",
    "  \"focus\": \"Examining effects of pruning on LLM safety against jailbreaking prompts\",\n",
    "  \"methodology\": \"Comparative analysis using a new dataset of malicious tasks\",\n",
    "  \"key_findings\": \"Pruning can increase resistance to jailbreaking prompts\",\n",
    "  \"publication_date\": \"2024-01-19\"\n",
    "}\n",
    "\n",
    "rl_for_qa = {\n",
    "  \"title\": \"Reinforcement Learning for Question Answering in Programming Domain Using Public Community Scoring as Human Feedback\",\n",
    "  \"authors\": [\"Alexey Gorbatovski\", \"Sergey Kovalchuk\"],\n",
    "  \"affiliations\": [\"ITMO University\", \"Huawei\"],\n",
    "  \"focus\": \"Enhancing GPT Neo 125M's performance in Community QA through RLHF\",\n",
    "  \"methodology\": [\"Use of RLHF with community scoring\", \"Fine-tuning with Proximal Policy Optimization\"],\n",
    "  \"key_insights\": \"Limitations of traditional metrics for programming QA and need for domain-specific evaluation methods\",\n",
    "  \"publication_date\": \"2024-01-19\",\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "363d7806-98e8-426a-b682-fccb541bc0d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "apple_vision_pro_news = {\n",
    "  \"title\": \"Apple Vision Pro: Lack of Netflix YouTube App Store Tensions Threaten Device\",\n",
    "  \"source\": \"Bloomberg\",\n",
    "  \"date\": \"January 21, 2024\",\n",
    "  \"author\": \"Mark Gurman\",\n",
    "  \"main_points\": {\n",
    "    \"focus\": \"Issues surrounding the launch of Apple's Vision Pro headset\",\n",
    "    \"key_details\": [\n",
    "      \"The Vision Pro headset lacks key apps like Netflix and YouTube at launch.\",\n",
    "      \"Tensions with developers and app store policies have impacted app availability.\",\n",
    "      \"Developers are cautious about investing in the platform due to past experiences with other Apple platforms.\"\n",
    "    ],\n",
    "    \"implications\": \"The success of the Vision Pro may be hindered by a lack of support from major app developers and existing tensions with Apple.\"\n",
    "  }\n",
    "}\n",
    "\n",
    "crypto_article = {\n",
    "  \"title\": \"2024 Crypto Crime Trends from Chainalysis\",\n",
    "  \"publication_date\": \"January 24, 2024\",\n",
    "  \"overview\": {\n",
    "    \"focus\": \"Analysis of cryptocurrency-related crimes in 2024\",\n",
    "    \"key_trends\": [\n",
    "      \"Illicit cryptocurrency activity experienced a significant drop in value.\",\n",
    "      \"Rise in ransomware and darknet market activities, despite overall decrease in crypto crime.\",\n",
    "      \"Shift in the types of assets involved in cryptocurrency-based crimes, with a notable increase in stablecoins usage.\"\n",
    "    ],\n",
    "    \"key_findings\": [\n",
    "      \"Scamming and stolen funds decreased, with a notable shift in scam tactics.\",\n",
    "      \"Ransomware and darknet market revenues increased, indicating persistent challenges.\",\n",
    "      \"Sanctions-related transactions formed a significant portion of illicit activity.\"\n",
    "    ],\n",
    "    \"implications\": \"The evolving landscape of crypto crime highlights the need for robust regulatory frameworks and advanced monitoring tools.\"\n",
    "  }\n",
    "}\n",
    "\n",
    "gov_crypto_stuff = {\n",
    "  \"title\": \"The Effectiveness of Economic Sanctions At Risk from Digital Asset Growth\",\n",
    "  \"source\": \"U.S. Government Accountability Office\",\n",
    "  \"publication_date\": \"January 24, 2024\",\n",
    "  \"overview\": {\n",
    "    \"focus\": \"Impact of digital assets on the effectiveness of U.S. economic sanctions\",\n",
    "    \"key_issues\": [\n",
    "      \"Digital assets are increasingly used to evade U.S. sanctions.\",\n",
    "      \"Sanctioned entities use cryptocurrencies to hide transactions and fund illicit activities.\"\n",
    "    ],\n",
    "    \"actions_taken\": [\n",
    "      \"DOJ and Treasury have taken actions against entities using cryptocurrencies to evade sanctions.\",\n",
    "      \"Federal agencies work with international partners to build investigative capacity against illicit uses of digital assets.\"\n",
    "    ],\n",
    "    \"implications\": \"Rising use of digital assets could weaken the impact of U.S. sanctions, but advancements in tracing transactions may mitigate risks.\"\n",
    "  }\n",
    "}\n",
    "\n",
    "imf_crypto_blog = {\n",
    "  \"title\": \"Crypto Needs Comprehensive Policies to Protect Economies and Investors\",\n",
    "  \"source\": \"International Monetary Fund (IMF)\",\n",
    "  \"publication_date\": \"July 18, 2023\",\n",
    "  \"authors\": [\"Tobias Adrian\", \"Dong He\", \"Arif Ismail\", \"Marina Moretti\"],\n",
    "  \"overview\": {\n",
    "    \"focus\": \"Establishment of effective policies for cryptocurrencies\",\n",
    "    \"key_issues\": [\n",
    "      \"Need for clearer policies in light of crypto asset failures and exchange collapses.\",\n",
    "      \"Importance of robust safeguards against fraud and misconduct in the crypto sector.\"\n",
    "    ],\n",
    "    \"recommendations\": [\n",
    "      \"Integration of cryptocurrencies within existing financial regulatory regimes.\",\n",
    "      \"Clear legal treatment and consistent regulatory approaches to crypto assets.\"\n",
    "    ],\n",
    "    \"implications\": \"Effective crypto policies are crucial to protect investors and ensure financial stability amid the digitalization of economies.\"\n",
    "  }\n",
    "}\n",
    "\n",
    "kraken_blog = {\n",
    "  \"title\": \"Crypto and the Age of Alternative Payroll\",\n",
    "  \"author\": \"Pranesh Anthapur\",\n",
    "  \"publication_date\": \"January 24, 2024\",\n",
    "  \"source\": \"Kraken Blog\",\n",
    "  \"overview\": {\n",
    "    \"focus\": \"Integration of cryptocurrencies in payroll systems\",\n",
    "    \"key_points\": [\n",
    "      \"Offering diverse payment options including cryptocurrencies and NFTs.\",\n",
    "      \"Benefits of crypto payroll for employers and employees.\",\n",
    "      \"Growing interest among younger generations for payment in cryptocurrencies.\",\n",
    "      \"Potential for global inclusivity and financial sovereignty for employees.\"\n",
    "    ],\n",
    "    \"implications\": \"The shift towards digital assets in payroll systems reflects changing employment trends and could impact future recruitment and retention strategies.\"\n",
    "  }\n",
    "}\n",
    "\n",
    "some_politics_thing = {\n",
    "  \"title\": \"Politics in Stereo: The Readout With Allegra Stratton\",\n",
    "  \"source\": \"Bloomberg\",\n",
    "  \"publication_date\": \"January 24, 2024\",\n",
    "  \"author\": \"Allegra Stratton\",\n",
    "  \"overview\": {\n",
    "    \"focus\": \"Analysis of political scenarios in the UK and US\",\n",
    "    \"key_points\": [\n",
    "      \"Discussion of the political environment in the UK, with a focus on the Conservative Party's strategies.\",\n",
    "      \"Comparative analysis of political strategies in the UK and the US.\",\n",
    "      \"Examination of political figures' tactics and public reception.\"\n",
    "    ],\n",
    "    \"implications\": \"The document provides insights into the political dynamics in the UK and US, highlighting similarities and differences in approaches and public perception.\"\n",
    "  }\n",
    "}\n",
    "\n",
    "streaming_piracy = {\n",
    "  \"title\": \"Pirate TV Websites Steal Content and Revenue From Netflix Disney+\",\n",
    "  \"source\": \"Bloomberg\",\n",
    "  \"publication_date\": \"January 24, 2024\",\n",
    "  \"author\": \"Thomas Buckley\",\n",
    "  \"overview\": {\n",
    "    \"focus\": \"Impact of pirate TV websites on streaming services\",\n",
    "    \"key_points\": [\n",
    "      \"Pirate TV websites are significantly impacting Netflix and Disney+ revenue.\",\n",
    "      \"These websites use software to hijack streams from legitimate video platforms.\",\n",
    "      \"The illegal services are estimated to earn about $2 billion, posing a serious threat to the profitability of legal streaming services.\"\n",
    "    ],\n",
    "    \"implications\": \"The rise of piracy in the streaming industry highlights the need for stronger digital rights management and legal measures to protect content creators and services.\"\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "504785d1-3db1-4720-808d-72c2f36deca2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dpo_github = {\n",
    "  \"title\": \"Direct Preference Optimization\",\n",
    "  \"source\": \"GitHub\",\n",
    "  \"repository\": \"eric-mitchell/direct-preference-optimization\",\n",
    "  \"overview\": {\n",
    "    \"description\": \"Reference implementation for Direct Preference Optimization (DPO), a method for training language models from preference data.\",\n",
    "    \"features\": [\n",
    "      \"Supports various causal HuggingFace models.\",\n",
    "      \"Includes supervised fine-tuning and preference learning stages.\",\n",
    "      \"Compatible with different datasets and easy to add new ones.\"\n",
    "    ],\n",
    "    \"key_components\": [\n",
    "      \"train.py: Main training entry point.\",\n",
    "      \"trainers.py: Contains trainer classes with multi-GPU logic.\",\n",
    "      \"utils.py: Convenience functions.\",\n",
    "      \"preference_datasets.py: Dataset processing logic.\"\n",
    "    ],\n",
    "    \"languages\": \"Python\",\n",
    "    \"license\": \"Apache-2.0\"\n",
    "  }\n",
    "}\n",
    "\n",
    "litellm = {\n",
    "  \"title\": \"LiteLLM\",\n",
    "  \"repository\": \"BerriAI/litellm\",\n",
    "  \"description\": \"A project for calling all LLM APIs using the OpenAI format, supporting various models and providers like Bedrock, Azure, OpenAI, Cohere, Anthropic, Ollama, Sagemaker, HuggingFace, and Replicate.\",\n",
    "  \"features\": [\n",
    "    \"Supports 100+ LLMs including GPT-3.5-turbo, Cohere, and others.\",\n",
    "    \"Offers streaming model responses with support for all models.\",\n",
    "    \"Includes proxy server configurations for different LLM providers.\"\n",
    "  ],\n",
    "  \"usage\": \"Simplifies making API calls to various LLMs and managing responses.\",\n",
    "  \"license\": \"MIT\",\n",
    "  \"contributors\": \"Contributions from various developers\",\n",
    "  \"repository_url\": \"https://github.com/BerriAI/litellm\"\n",
    "}\n",
    "\n",
    "pytorch = {\n",
    "  \"title\": \"PyTorch\",\n",
    "  \"repository\": \"pytorch/pytorch\",\n",
    "  \"description\": \"A Python package for tensors and dynamic neural networks with strong GPU acceleration.\",\n",
    "  \"key_features\": [\n",
    "    \"Tensor computation (like NumPy) with strong GPU acceleration.\",\n",
    "    \"Deep neural networks built on a tape-based autograd system.\"\n",
    "  ],\n",
    "  \"languages_used\": [\"Python\", \"C++\", \"Cuda\", \"C\", \"Objective-C++\", \"CMake\", \"Other\"],\n",
    "  \"license\": \"Unknown\",\n",
    "  \"contributions\": \"Supported by a community of engineers and researchers with major contributions from various individuals.\",\n",
    "  \"repository_url\": \"https://github.com/pytorch/pytorch\"\n",
    "}\n",
    "\n",
    "tinygrad = {\n",
    "  \"title\": \"tinygrad\",\n",
    "  \"repository\": \"tinygrad/tinygrad\",\n",
    "  \"description\": \"A minimalist deep learning library positioned between PyTorch and micrograd.\",\n",
    "  \"key_features\": [\n",
    "    \"Implementation of neural networks using a simple autograd/tensor library.\",\n",
    "    \"Supports various accelerators like CPU, GPU, and more with around 25 low-level ops.\",\n",
    "    \"Includes examples like running LLaMA and Stable Diffusion.\"\n",
    "  ],\n",
    "  \"languages\": [\"Python\", \"C++\", \"C\", \"Others\"],\n",
    "  \"license\": \"Unknown\",\n",
    "  \"repository_url\": \"https://github.com/tinygrad/tinygrad\"\n",
    "}\n",
    "\n",
    "vanna = {\n",
    "  \"title\": \"Vanna\",\n",
    "  \"repository\": \"vanna-ai/vanna\",\n",
    "  \"description\": \"A framework for accurate Text-to-SQL Generation via LLMs using RAG.\",\n",
    "  \"overview\": {\n",
    "    \"features\": [\n",
    "      \"Enables chatting with SQL databases.\",\n",
    "      \"Uses RAG for generating SQL queries from natural language inputs.\",\n",
    "      \"Aims to provide accurate SQL generation.\"\n",
    "    ],\n",
    "    \"usage\": \"Can be used for various applications where SQL query generation from natural language is needed.\",\n",
    "    \"languages\": [\"Python\"],\n",
    "    \"license\": \"MIT\",\n",
    "    \"repository_url\": \"https://github.com/vanna-ai/vanna\"\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "clarity = {\n",
    "  \"title\": \"Clarity\",\n",
    "  \"repository\": \"mckaywrigley/clarity\",\n",
    "  \"description\": \"A simple interface designed to fetch up-to-date information from the web, using OpenAI's API to generate answers.\",\n",
    "  \"key_features\": [\n",
    "    \"Fetches relevant information from the web based on user queries.\",\n",
    "    \"Uses OpenAI's API to process queries and generate accurate answers.\",\n",
    "    \"Simple and user-friendly interface.\"\n",
    "  ],\n",
    "  \"usage\": \"Helps users obtain accurate and current information quickly.\",\n",
    "  \"license\": \"MIT\",\n",
    "  \"repository_url\": \"https://github.com/mckaywrigley/clarity\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "649c3857-66c0-4bb5-a1b9-3e63cb612005",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# format is <pdf_file_name> : corresponding structured dict\n",
    "structured_data = {\n",
    "    \"clarity.pdf\": clarity,\n",
    "    \"dpo_github.pdf\": dpo_github,\n",
    "    \"litellm.pdf\": litellm,\n",
    "    \"pytorch.pdf\": pytorch,\n",
    "    \"tinygrad.pdf\": tinygrad,\n",
    "    \"vanna.pdf\": vanna\n",
    "}\n",
    "\n",
    "# path to the folder of pdfs you want to read in\n",
    "dir_path = \"github_pdfs\"\n",
    "\n",
    "\n",
    "output_csv_name = 'github_pdfs_data_cleaned.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54a6fd4a-5d2d-410f-b352-f1b30aebcf3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "import pandas as pd\n",
    "\n",
    "def load_pdfs(dir_path: str) -> Dict[str, str]:\n",
    "    file_contents = {}\n",
    "    files = [f for f in os.listdir(dir_path) if f.endswith('.pdf')]\n",
    "    for file in files:\n",
    "        loader = PyPDFLoader(os.path.join(dir_path, file))\n",
    "        pages = loader.load_and_split()\n",
    "        full_text = ' '.join([page.page_content for page in pages])\n",
    "        full_text = full_text.replace(\"'\", \"\")  # Removing all apostrophes\n",
    "        file_contents[file] = full_text\n",
    "    return file_contents\n",
    "\n",
    "pdf_contents = load_pdfs(dir_path)\n",
    "\n",
    "df_list = []\n",
    "for file_name, raw_content in pdf_contents.items():\n",
    "    structured_info = structured_data.get(file_name, {})\n",
    "    df_list.append({'raw': raw_content.replace(\"'\", \"\"), 'structured': structured_info})  # Removing apostrophes\n",
    "\n",
    "combined_df = pd.DataFrame(df_list)\n",
    "combined_df.to_csv(output_csv_name, index=False)\n",
    "\n",
    "print(combined_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05364015-d516-4ccf-bf75-1cf3cef300fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "kaggle_data = pd.read_csv('/Users/wbryan/Desktop/Github/dbks_hackathon_DBUBurners/kaggle_data/OSHA HSE DATA_ALL ABSTRACTS 15-17_FINAL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "919c219e-7955-40ae-844a-06b6573b2785",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_structured_dataset(df):\n",
    "    new_df = pd.DataFrame()\n",
    "    \n",
    "    new_df['raw'] = df['Abstract Text']\n",
    "\n",
    "    new_df['structured'] = df.drop('Abstract Text', axis=1).apply(lambda row: row.to_dict(), axis=1)\n",
    "\n",
    "    return new_df\n",
    "\n",
    "structured_df = create_structured_dataset(kaggle_data)\n",
    "structured_df.to_csv('osha_dataset.csv')\n",
    "print(structured_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70ae6a0d-f160-49c0-b50d-ca00ca5593cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "dataset_building",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3.11.4 ('dbkshack': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08f1c185a11ec3bb7ec5e07ee737b42c8afcf80b878d81e864da17ed0d390b00"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
