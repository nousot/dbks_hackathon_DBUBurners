{'quant_method': <QuantizationMethod.GPTQ: 'gptq'>, 'bits': 4, 'tokenizer': None, 'dataset': None, 'group_size': 128, 'damp_percent': 0.1, 'desc_act': True, 'sym': True, 'true_sequential': True, 'use_cuda_fp16': False, 'model_seqlen': None, 'block_name_to_quantize': None, 'module_name_preceding_first_block': None, 'batch_size': 1, 'pad_token_id': None, 'use_exllama': True, 'max_input_length': None, 'exllama_config': {'version': <ExllamaVersion.ONE: 1>}, 'cache_block_outputs': True, 'modules_in_block_to_quantize': None}