raw,structured
"1/26/24, 12:13â€¯PMtinygrad/tinygrad: You like pytorch? You like micrograd? You love tinygrad! 
â¤Page 1 of 6https://github.com/tinygrad/tinygrad
tinygradPublic27 Branches5 Tags
Go to file
t
Go to file
Add file
Code
oâ€¦bc92c4cÂ Â·Â 2 hours ago3,482 Commits.github/worâ€¦onnx Einsum, Cuâ€¦2 hours agodisassemblâ€¦start Qualcomm â€¦last monthdocsfix fuzz_linearizerâ€¦last weekexamplescifar move Globalâ€¦3 days agoextraonnx Einsum, Cuâ€¦2 hours agoopenpilotremove numpy frâ€¦2 weeks agotestonnx Einsum, Cuâ€¦2 hours agotinygradhip launch speed â€¦19 hours ago.editorconfigRevert ""update eâ€¦6 months ago.gitignoreNo extra vars call â€¦2 weeks ago.pre-commâ€¦remove numpy frâ€¦2 weeks ago.pylintrcruff checks the mâ€¦last month.tokeignoreAdd a quick start â€¦8 months agoLICENSEUpdated LICENSâ€¦9 months agoREADME.mdcomplex PRs will â€¦last weekmypy.iniback to 6.54GB fâ€¦2 months agopush_pypi.â€¦push pypi4 years agoruff.tomlruff checks the mâ€¦last monthrun_multibâ€¦convert $@ to ""$@â€¦6 months agoAboutYou like pytorch? You like micrograd?You love tinygrad! 
â¤
 Readme MIT license Activity Custom properties 22.1k stars 272 watching 2.9k forksReport repositoryReleases 
5tinygrad 0.8.0Latest2 weeks ago+ 4 releasesPackagesNo packages publishedContributors
269
+ 255 contributorsLanguagesPython79.0% C17.9%C++1.4% Objective-C++0.9%
tinygrad/tinygrad
Type / to search
CodeIssues
73Pull requests
37DiscussionsActionsSecurityInsights
Â masterchenyuxyz 1/26/24, 12:13â€¯PMtinygrad/tinygrad: You like pytorch? You like micrograd? You love tinygrad! 
â¤Page 2 of 6https://github.com/tinygrad/tinygradsetup.pyRemove webgpu, â€¦3 weeks agostrip_whiteâ€¦strip whitespace7 months agosz.pyfix some long lineâ€¦3 weeks ago
tinygrad: For something between PyTorch and karpathy/micrograd. Maintained by tiny corp.Homepage | Documentation | Examples | Showcase | DiscordStarsStars22k22k Unit TestsUnit Testspassingpassing chatchat1292 online1292 onlineThis may not be the best deep learning framework, but it is a deep learning framework.Due to its extreme simplicity, it aims to be the easiest framework to add new accelerators to, with support for bothinference and training. If XLA is CISC, tinygrad is RISC.tinygrad is still alpha software, but we raised some money to make it good. Someday, we will tape out chips.FeaturesLLaMA and Stable Diffusiontinygrad can run LLaMA and Stable Diffusion!LazinessTry a matmul. See how, despite the style, it is fused into one kernel with the power of laziness.Shell0.4% Assembly0.1%Other0.3%
READMEMIT license 1/26/24, 12:13â€¯PMtinygrad/tinygrad: You like pytorch? You like micrograd? You love tinygrad! 
â¤Page 3 of 6https://github.com/tinygrad/tinygradAnd we can change DEBUG to 4 to see the generated code.Neural networksAs it turns out, 90% of what you need for neural networks are a decent autograd/tensor library. Throw in anoptimizer, a data loader, and some compute, and you have all you need.
See examples/beautiful_mnist.py for the full version that gets 98% in ~5 secondsAcceleratorstinygrad already supports numerous accelerators, including:
And it is easy to add more! Your accelerator of choice only needs to support a total of ~25 low level ops. MoreDEBUG=3 python3 -c ""from tinygrad import Tensor;N = 1024; a, b = Tensor.rand(N, N), Tensor.rand(N, N);c = (a.reshape(N, 1, N) * b.T.reshape(1, N, N)).sum(axis=2);print((c.numpy() - (a.numpy() @ b.numpy())).mean())""
from tinygrad import Tensor, nnclass LinearNet:  def __init__(self):    self.l1 = Tensor.kaiming_uniform(784, 128)    self.l2 = Tensor.kaiming_uniform(128, 10)  def __call__(self, x:Tensor) -> Tensor:    return x.flatten(1).dot(self.l1).relu().dot(self.l2)model = LinearNet()optim = nn.optim.Adam([model.l1, model.l2], lr=0.001)x, y = Tensor.rand(4, 1, 28, 28), Tensor([2,4,3,7])  # replace with real mnist dataloaderfor i in range(10):  optim.zero_grad()  loss = model(x).sparse_categorical_crossentropy(y).backward()  optim.step()  print(i, loss.item())
 CPU
 GPU (OpenCL)
 C Code (Clang)
 LLVM
 METAL
 CUDA
 PyTorch
 HIP 1/26/24, 12:13â€¯PMtinygrad/tinygrad: You like pytorch? You like micrograd? You love tinygrad! 
â¤Page 4 of 6https://github.com/tinygrad/tinygradAnd it is easy to add more! Your accelerator of choice only needs to support a total of ~25 low level ops. Moreinformation can be found in the documentation for adding new accelerators.InstallationThe current recommended way to install tinygrad is from source.From sourceDirect (master)DocumentationDocumentation along with a quick start guide can be found in the docs/ directory.Quick example comparing to PyTorch
The same thing but in PyTorch:
Contributinggit clone https://github.com/tinygrad/tinygrad.gitcd tinygradpython3 -m pip install -e .python3 -m pip install git+https://github.com/tinygrad/tinygrad.git
from tinygrad import Tensorx = Tensor.eye(3, requires_grad=True)y = Tensor([[2.0,0,-2.0]], requires_grad=True)z = y.matmul(x).sum()z.backward()print(x.grad.numpy())  # dz/dxprint(y.grad.numpy())  # dz/dyimport torchx = torch.eye(3, requires_grad=True)y = torch.tensor([[2.0,0,-2.0]], requires_grad=True)z = y.matmul(x).sum()z.backward()print(x.grad.numpy())  # dz/dxprint(y.grad.numpy())  # dz/dy 1/26/24, 12:13â€¯PMtinygrad/tinygrad: You like pytorch? You like micrograd? You love tinygrad! 
â¤Page 5 of 6https://github.com/tinygrad/tinygradContributingThere has been a lot of interest in tinygrad lately. Following these guidelines will help your PR get accepted.Well start with what will get your PR closed with a pointer to this section:No code golf! While low line count is a guiding light of this project, anything that remotely looks like code golfwill be closed. The true goal is reducing complexity and increasing readability, and deleting \ns doesnothing to help with that.All docs and whitespace changes will be closed unless you are a well-known contributor. The people writingthe docs should be those who know the codebase the absolute best. People who have not demonstrated thatshouldnt be messing with docs. Whitespace changes are both useless and carry a risk of introducing bugs.Anything you claim is a ""speedup"" must be benchmarked. In general, the goal is simplicity, so even if your PRmakes things marginally faster, you have to consider the tradeoff with maintainablity and readablity.In general, the code outside the core tinygrad/ folder is not well tested, so unless the current code there isbroken, you shouldnt be changing it.If your PR looks ""complex"", is a big diff, or adds lots of lines, it wont be reviewed or merged. Considerbreaking it up into smaller PRs that are individually clear wins. A common pattern I see is prerequisiterefactors before adding new functionality. If you can (cleanly) refactor to the point that the feature is a 3 linechange, this is great, and something easy for us to review.Now, what we want:Bug fixes (with a regression test) are great! This library isnt 1.0 yet, so if you stumble upon a bug, fix it, writea test, and submit a PR, this is valuable work.Solving bounties! tinygrad offers cash bounties for certain improvements to the library. All new code shouldbe high quality and well tested.Features. However, if you are adding a feature, consider the line tradeoff. If its 3 lines, theres less of a bar ofusefulness it has to meet over something thats 30 or 300 lines. All features must have regression tests. Ingeneral with no other constraints, your features API should match torch or numpy.Refactors that are clear wins. In general, if your refactor isnt a clear win it will be closed. But some refactorsare amazing! Think about readability in a deep core sense. A whitespace change or moving a few functionsaround is useless, but if you realize that two 100 line functions can actually use the same 110 line functionwith arguments while also improving readability, this is a big win.Tests/fuzzers. If you can add tests that are non brittle, they are welcome. We have some fuzzers in here too,and theres a plethora of bugs that can be found with them and by improving them. Finding bugs, evenwriting broken tests (that should pass) with @unittest.expectedFailure is great. This is how we makeprogress.Dead code removal from core tinygrad/ folder. We dont care about the code in extra, but removing deadcode from the core library is great. Less for new people to read and be confused by.Running testsYou should install the pre-commit hooks with pre-commit install. This will run the linter, mypy, and a subset ofthe tests on every commit.For more examples on how to run the full test suite please refer to the CI workflow. 1/26/24, 12:13â€¯PMtinygrad/tinygrad: You like pytorch? You like micrograd? You love tinygrad! 
â¤Page 6 of 6https://github.com/tinygrad/tinygradFor more examples on how to run the full test suite please refer to the CI workflow.Some examples of running tests locally:python3 -m pip install -e .[testing]  # install extra deps for testingpython3 test/test_ops.py                # just the ops testspython3 -m pytest test/                 # whole test suite","{'title': 'tinygrad', 'repository': 'tinygrad/tinygrad', 'description': 'A minimalist deep learning library positioned between PyTorch and micrograd.', 'key_features': ['Implementation of neural networks using a simple autograd/tensor library.', 'Supports various accelerators like CPU, GPU, and more with around 25 low-level ops.', 'Includes examples like running LLaMA and Stable Diffusion.'], 'languages': ['Python', 'C++', 'C', 'Others'], 'license': 'Unknown', 'repository_url': 'https://github.com/tinygrad/tinygrad'}"
"1/26/24, 12:08â€¯PMeric-mitchell/direct-preference-optimization: Reference implementation for DPO (Direct Preference Optimization)
Page 1 of 9https://github.com/eric-mitchell/direct-preference-optimization
direct-preference-optimizationPublic1 Branch0 Tags
Go to file
t
Go to file
Add file
Code
Uâ€¦f8b8c0fÂ Â·Â last month16 Commitscoâ€¦Rename â€¦2 months agoLIâ€¦Initial coâ€¦7 months agoRâ€¦Updatedâ€¦last monthprâ€¦Fixed tokâ€¦7 months agoreâ€¦Initial coâ€¦7 months agotrâ€¦Added câ€¦2 months agotrâ€¦Rename â€¦2 months agoutiâ€¦Updatedâ€¦6 months agoDPO: Direct Preference OptimizationNew: in addition to the original DPO algorithm, this repo now supportsconservative DPO and IPO.AboutReference implementation for DPO(Direct Preference Optimization) Readme Apache-2.0 license Activity 1.2k stars 13 watching 86 forksReport repositoryReleasesNo releases publishedPackagesNo packages publishedLanguagesPython100.0%
eâ€¦/dâ€¦
Type / to search
CodeIssues
21Pull requestsActionsProjectsSecurityInsights
Â maineric-mitchell
READMEApache-2.0 license 1/26/24, 12:08â€¯PMeric-mitchell/direct-preference-optimization: Reference implementation for DPO (Direct Preference Optimization)
Page 2 of 9https://github.com/eric-mitchell/direct-preference-optimizationFor conservative DPO, you just need to additionally pass the parameterloss.label_smoothing=X for some X between 0 and 0.5 whenperforming DPO training (0 gives the original DPO loss). This parameter isessentially the conservativeness parameter, i.e., the fraction of the trainingpreference data that is incorrect (flipped preference direction). Startingwith something like 0.1 might be reasonable, but I havent tested this yet(and it will depend on the preference dataset).For IPO, just pass loss=ipo and loss.beta=X for some non-negative X(same as with DPO/conservative DPO).What is this repo?This repo includes a reference implementation of the DPO algorithm fortraining language models from preference data, as described in the paperDirect Preference Optimization: Your Language Model is Secretly a RewardModel.The code here supports any causal HuggingFace model- look at ourexamples in config/model to add your own. Adding your own datasets isalso easy. See the README section on adding datasets.The DPO pipeline has two stages:1. Run supervised fine-tuning (SFT) on the dataset(s) of interest.2. Run preference learning on the model from step 1, using preferencedata (ideally from the same distribution as the SFT examples).The files in this repo are:train.py: the main entry point for training (either SFT or DPOpreference-based training)trainers.py: the trainer classes (e.g., implementing the loop oflearning as well as multi-GPU logic)utils.py: some convenience functions used by multiple other filespreference_datasets.py: dataset processing logic for both SFT andDPO preference-based training; this is where youll need to makesome additions to train on your own data 1/26/24, 12:08â€¯PMeric-mitchell/direct-preference-optimization: Reference implementation for DPO (Direct Preference Optimization)
Page 3 of 9https://github.com/eric-mitchell/direct-preference-optimizationRunning SFTFor DPO, the SFT stage essentially ensures that the preference data wetrain on is in-distribution for our policy before we actually do the learningfrom preferences part.Run SFT for Pythia 6.9B on Anthropic-HH data with batch size 64:
Run SFT for a custom model (for example, Llama at a local path) onAnthropic-HH + Stanford Human Preference data with batch size 64:
Note: Since were not using one of our predefined model configs, wealso need to pass model.block_name to tell FSDP what modules towrap.By default, evaluation will run every 20k examples. You can change this argwith eval_every arg. If you dont pass sample_during_eval=false,sampling will happen during each eval as well.To run a different model, either add a new model config to config/model,or use the blank_model option for model and passmodel.name_or_path (and model.block_name if training with FSDPtrainer) explicitly. For example, for GPT-2, this would look like:python -u train.py model=pythia69 datasets=[hh] loss=sft exp_name=anthropic_dpo_pythia69 gradient_accumulation_steps=2 batch_size=64 eval_batch_size=32 trainer=FSDPTrainer sample_during_eval=false
python -u train.py model=blank_model model.name_or_path=/PATH/TO/LLAMA/WEIGHTS model.block_name=LlamaDecoderLayer datasets=[hh,shp] loss=sft exp_name=anthropic_shp_sft_llama_7b gradient_accumulation_steps=2 batch_size=64 eval_batch_size=32 trainer=FSDPTrainer sample_during_eval=false 1/26/24, 12:08â€¯PMeric-mitchell/direct-preference-optimization: Reference implementation for DPO (Direct Preference Optimization)
Page 4 of 9https://github.com/eric-mitchell/direct-preference-optimizationRunning DPOTo run DPO, use the same command as SFT, but pass loss=dpo,loss.beta=DESIRED_BETA (0.1-0.5 is a good starting point), andmodel.archive=/path/to/checkpoint/from/sft/step-XXXX/policy.pt.If SFT completed successfully, you should also have a/.../LATEST/policy.pt from the end of training.Run DPO on Pythia 6.9B with effective batch size 64:
Note: eval_every is measured in examples.A complete exampleLets work through a complete example training pythia 2.8B on theAnthropic-HH dataset.See sample wandb outputs for this example here (tagged readme-example).Step 1: Set up environmentFirst, create a virtualenv and install the dependencies. Python 3.8+ isrecommended.python -u train.py ... model=blank_model model.name_or_path=gpt2-xl model.block=GPT2Block
python -u train.py model=pythia69 datasets=[hh] loss=dpo loss.beta=0.1 model.archive=/path/to/checkpoint/from/sft/step-XXXX/policy.pt exp_name=anthropic_dpo_pythia69 gradient_accumulation_steps=2 batch_size=32 eval_batch_size=32 trainer=FSDPTrainer sample_during_eval=false 1/26/24, 12:08â€¯PMeric-mitchell/direct-preference-optimization: Reference implementation for DPO (Direct Preference Optimization)
Page 5 of 9https://github.com/eric-mitchell/direct-preference-optimizationStep 2: Run SFTWell take advantage of FSDPs mixed precision in bfloat16 to speed uptraining; we usually see about a 50% speedup. By default, SFT will run for asingle epoch over a mixture of the selected datasets. Datasets will bedownloaded on the fly and cached locally.
Note: this command is run on a machine with 4 80GB A100s; on thishardware, SFT takes about 1hr 30min. If you have less computeavailable, you might need to increase the number of gradientaccumulation steps, and SFT will take longer.See sample wandb outputs for the SFT step here.Step 3: Run DPOCheck either wandb (if enabled, it is by default) or your output log to findthe local run directory. To run DPO, youll need the path to the final weights,which will look something like/some/cache/dir/YOUR_USERNAME/pythia28_hh_sft_bf16_2023-06-21_16-58-17_973996/LATEST/policy.pt. The LATEST directory containsthe final set of weights from the end of training.python3 -m venv envsource env/bin/activatepip install -r requirements.txt
python -u train.py model=pythia28 datasets=[hh] loss=sft exp_name=anthropic_dpo_pythia28 gradient_accumulation_steps=2 batch_size=64 eval_batch_size=32 trainer=FSDPTrainer sample_during_eval=false model.fsdp_policy_mp=bfloat16 1/26/24, 12:08â€¯PMeric-mitchell/direct-preference-optimization: Reference implementation for DPO (Direct Preference Optimization)
Page 6 of 9https://github.com/eric-mitchell/direct-preference-optimizationOn 4 80GB A100s, DPO training took about 2hrs 45min.See sample wandb outputs for the DPO step here.Customizing trainingThe options for training are in config/config.yaml,config/model/blank_model.yaml, and config/loss/dpo.yaml. See thecomments in these files for more information on what they do.You can use one of the pre-configured models by passingmodel=some_model, where config/model/some_model.yaml exists. Wehave a few examples already given.If you want to use another model, just create a new config for that model(following our examples; it must be a .yaml file!), or usemodel=blank_model with model.name_or_path=NAME_OR_PATH, optionallymodel.tokenizer_name_or_path=TOKENIZER_NAME_OR_PATH if it isdifferent than the models name/path, andmodel.block_name=NAME_OF_TRANSFORMER_BLOCK (if you are using FSDP).The only other options you might want to change are the dpo loss options,which are loss.beta and loss.reference_free (seeconfig/loss/dpo.yaml).Trainer classesWe implement three different trainer classes in trainers.py:BasicTrainer: For multiple GPUs, naively partition the model amongthem. e.g., for two GPUs, the first half of the model layers will be onGPU 0, the second half will be on GPU 1. This trainer effectivelyincreases your available GPU memory without using multiple GPUs arepython -u train.py model=pythia28 datasets=[hh] loss=dpo loss.beta=0.1 exp_name=anthropic_dpo_pythia28 gradient_accumulation_steps=2 batch_size=64 eval_batch_size=32 trainer=FSDPTrainer sample_during_eval=false model.fsdp_policy_mp=bfloat16 model.archive=/path/to/archive/from/sft/LATEST/policy.pt 1/26/24, 12:08â€¯PMeric-mitchell/direct-preference-optimization: Reference implementation for DPO (Direct Preference Optimization)
Page 7 of 9https://github.com/eric-mitchell/direct-preference-optimizationonce for compute (so you get no speedup).FSDPTrainer: Use PyTorchs Fully Sharded Data Parallel (FSDP)implementation to shard each transformer block amongst availableGPUs. Should give a significant speedup over BasicTrainer withbatch size per GPU >1. The batch size per gpu is equal tobatch_size / (gradient_accumulation_steps * num_gpus). Youmay need to run ulimit -n 64000 in your launch script beforecalling train.py with this trainer; e.g., ulimit -n 64000; python train.py ....TensorParallelTrainer: Use PyTorch tensor parallelism (with thiswrapper) to shard each linear layer amongst available GPUs. Thistrainer is experimental, but should work.Warning: Sampling may be very slow for FSDPTrainer and especiallyTensorParallelTrainer (see this issue and this issue, respectively forFSDPTrainer and TensorParallelTrainer). Passingsample_during_eval=false is recommended for these trainers.Which trainer do I use?For single GPU training, use BasicTrainer. For many-GPU setups,FSDPTrainer will most likely be the best choice, though these haventbeen benchmarked yet.Adding new datasetsAdding new/custom datasets is easy, and shouldnt take more than 10minutes or so. Add your dataset to preference_datasets.py (weveimplemented Anthropic-HH, Stanford Human Preferences, andStackExchange as references). Follow our reference datasets (in thefunctions get_se(), get_shp(), get_hh()); you essentially need toreturn a dict mapping each prompt to another dict containing three values:responses: List[str]: the list of responses on which preferencesare givenpairs: List[Tuple[int]]: the preference pairs, where the first valuein each tuple is the preferred response and the second value is the 1/26/24, 12:08â€¯PMeric-mitchell/direct-preference-optimization: Reference implementation for DPO (Direct Preference Optimization)
Page 8 of 9https://github.com/eric-mitchell/direct-preference-optimizationdispreferred responsesft_target: str: the response to use for this prompt during SFT(this response may or may not be one of the values in responses)Once youve added your dataset, for example xyz, you can train on it bypassing it to datasets=[xyz] to an SFT or DPO train command.Make sure youve updated preference_datasets:get_dataset() toreturn your new dataset when its name is passed in!Tips for faster training on multipleGPUsFSDP is recommended for faster training when multiple GPUs are available.In general, you should try to use a batch size of at least 2 on each GPU (i.e.,batch_size // (grad_accumulation_steps * N_GPUS) is at least 2) tosee a speedup from FSDP compared to the BasicTrainer. One way to dothis is to use mixed precision. This repo implements mixed precisionthrough FSDP. Enable mixed precision (only supported for FSDPTrainer,currently) by passing model.fsdp_policy_mp=bfloat16 ormodel.fsdp_policy_mp=float16 (only bfloat16 has been tested).Another way to reduce memory usage is activation checkpointing (orgradient checkpointing), which can be enabled withactivation_checkpointing=true (also implemented only forFSDPTrainer). Activation checkpointing doesnt always increasethroughput, but if youre stuck at batch size per GPU of 1, its worth a try.See this article for more information about optimizing FSDP.Citing DPOIf DPO or this repository is useful in your own research, you can use thefollowing BibTeX entry: 1/26/24, 12:08â€¯PMeric-mitchell/direct-preference-optimization: Reference implementation for DPO (Direct Preference Optimization)
Page 9 of 9https://github.com/eric-mitchell/direct-preference-optimization@inproceedings{    rafailov2023direct,    title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model},    author={Rafael Rafailov and Archit Sharma and Eric Mitchell and Christopher D Manning and Stefano Ermon and Chelsea Finn},    booktitle={Thirty-seventh Conference on Neural Information Processing Systems},    year={2023},    url={https://arxiv.org/abs/2305.18290}}","{'title': 'Direct Preference Optimization', 'source': 'GitHub', 'repository': 'eric-mitchell/direct-preference-optimization', 'overview': {'description': 'Reference implementation for Direct Preference Optimization (DPO), a method for training language models from preference data.', 'features': ['Supports various causal HuggingFace models.', 'Includes supervised fine-tuning and preference learning stages.', 'Compatible with different datasets and easy to add new ones.'], 'key_components': ['train.py: Main training entry point.', 'trainers.py: Contains trainer classes with multi-GPU logic.', 'utils.py: Convenience functions.', 'preference_datasets.py: Dataset processing logic.'], 'languages': 'Python', 'license': 'Apache-2.0'}}"
"1/26/24, 12:16â€¯PMBerriAI/litellm: Call all LLM APIs using the OpenAI format. Use Bedrâ€¦re, Anthropic, Ollama, Sagemaker, HuggingFace, Replicate (100+ LLMs)
Page 1 of 8https://github.com/BerriAI/litellm
litellmPublic65 Branches70 Tags
Go to file
t
Go to file
Add file
Code
Merge â€¦4634f7bÂ Â·Â 1 hour ago6,176 Commits.circlecifix(utils.py): fix sagemaker aâ€¦yesterday.github(ci/cd) build ui, litellm on arâ€¦17 hours agocookbook(docs) misc/cookbook - Opâ€¦3 days agodistfix: syncing changes2 weeks agodockerRevert ""build(Dockerfile): mâ€¦3 weeks agodocs/my-website(docs) new gpt-4-0125-preâ€¦19 hours agolitellmfix print verbose take only oâ€¦1 hour agoteststest(test_keys.py): add delaâ€¦15 hours agoui(ui) dockerfile13 hours ago.env.examplefeat: added support for OPEâ€¦5 months ago.flake8chore: list all ignored flake8 â€¦last month.gitattributesignore ipynbs5 months ago.gitignore(chore) gitignore2 weeks ago.pre-commit-config.yâ€¦(feat) pre-commit check priâ€¦last monthDockerfilefix: fix proxy logginglast weekDockerfile.alpine(fix) alpine Docker image2 weeks agoDockerfile.databasebuild(dockerfile.database): â€¦2 weeks agoLICENSEInitial commit6 months agoREADME.mdUpdate README.md2 weeks agodocker-compose.yml(ci/cd) docker compose up â€¦17 hours agoentrypoint.sh(ci/cd) set litellm as entrypoiâ€¦2 weeks agomodel_prices_and_câ€¦(feat) add gpt-4-0125-previâ€¦19 hours agomypy.inifix(google_kms.py): supportâ€¦last monthpoetry.lock(fix) add app scheduler to pâ€¦3 days agoproxy_server_config.â€¦fix(utils.py): fix sagemaker aâ€¦yesterdayAboutCall all LLM APIs using the OpenAIformat. Use Bedrock, Azure, OpenAI,Cohere, Anthropic, Ollama, Sagemaker,HuggingFace, Replicate (100+ LLMs)litellm-api.up.railway.app/# openai # llm # langchain # llmops # anthropic# langchain-python Readme MIT license Activity Custom properties 4.9k stars 42 watching 477 forksReport repositoryReleases 
61v1.19.4Latest17 hours ago+ 60 releasesSponsor this projecthttps://buy.stripe.com/9AQ03Kd3P91â€¦Packages
3litellmlitellm-databaselitellm-uiUsed by 
738
+ 730Contributors
97
BerriAI/litellm
Type / to search
CodeIssues
204Pull requests
29DiscussionsActionsProjectsSecurityInsights
Â mainishaan-jaff 1/26/24, 12:16â€¯PMBerriAI/litellm: Call all LLM APIs using the OpenAI format. Use Bedrâ€¦re, Anthropic, Ollama, Sagemaker, HuggingFace, Replicate (100+ LLMs)
Page 2 of 8https://github.com/BerriAI/litellmpyproject.tomlbump: version 1.19.3 â†’ 1.19.418 hours agorequirements.txtbuild(requirements.txt): addâ€¦3 days agoretry_push.shbuild(Dockerfile): moves priâ€¦3 weeks agoschema.prisma(feat) add cache_key in speâ€¦2 days agotemplate.yamlUse -function for naming.2 months ago
!
 LiteLLMCall all LLM APIs using the OpenAI format [Bedrock, Huggingface, VertexAI, TogetherAI, Azure, OpenAI, etc.]OpenAI Proxy Serverpypipypiv1.19.4v1.19.4  Y CombinatorW23 Chat onWhatsApp Chat onDiscordLiteLLM manages:Translate inputs to providers completion, embedding, and image_generation endpointsConsistent output, text responses will always be available at [choices][0][message][content]Retry/fallback logic across multiple deployments (e.g. Azure/OpenAI) - RouterJump to OpenAI Proxy DocsJump to Supported LLM ProvidersUsage (Docs)ImportantLiteLLM v1.0.0 now requires openai>=1.0.0. Migration guide hereOpen in ColabOpen in Colab
+ 83 contributorsDeployments
500+ Preview 2 minutes ago Production 1 hour ago main - webdis-8n6t+ more deploymentsLanguagesPython97.5% HTML2.3%Other0.2%
pip install litellmfrom litellm import completionimport os## set ENV variables os.environ[""OPENAI_API_KEY""] = ""your-openai-key"" os.environ[""COHERE_API_KEY""] = ""your-cohere-key"" messages = [{ ""content"": ""Hello, how are you?"",""role"": ""user""}]# openai callresponse = completion(model=""gpt-3.5-turbo"", messages=messages)# cohere callresponse = completion(model=""command-nightly"", messages=messages)print(response)READMEMIT license 1/26/24, 12:16â€¯PMBerriAI/litellm: Call all LLM APIs using the OpenAI format. Use Bedrâ€¦re, Anthropic, Ollama, Sagemaker, HuggingFace, Replicate (100+ LLMs)
Page 3 of 8https://github.com/BerriAI/litellmAsync (Docs)
Streaming (Docs)liteLLM supports streaming the model response back, pass stream=True to get a streaming iterator in response.Streaming is supported for all models (Bedrock, Huggingface, TogetherAI, Azure, OpenAI, etc.)
Logging Observability (Docs)LiteLLM exposes pre defined callbacks to send data to Langfuse, DynamoDB, s3 Buckets, LLMonitor, Helicone, Promptlayer, Traceloop,Slack
OpenAI Proxy - (Docs)Track spend across multiple projects/peopleThe proxy provides:1. Hooks for auth2. Hooks for loggingfrom litellm import acompletionimport asyncioasync def test_get_response():    user_message = ""Hello, how are you?""    messages = [{""content"": user_message, ""role"": ""user""}]    response = await acompletion(model=""gpt-3.5-turbo"", messages=messages)    return responseresponse = asyncio.run(test_get_response())print(response)
from litellm import completionresponse = completion(model=""gpt-3.5-turbo"", messages=messages, stream=True)for part in response:    print(part.choices[0].delta.content or """")# claude 2response = completion(claude-2, messages, stream=True)for part in response:    print(part.choices[0].delta.content or """")
from litellm import completion## set env variables for logging toolsos.environ[""LANGFUSE_PUBLIC_KEY""] = """"os.environ[""LANGFUSE_SECRET_KEY""] = """"os.environ[""LLMONITOR_APP_ID""] = ""your-llmonitor-app-id""os.environ[""OPENAI_API_KEY""]# set callbackslitellm.success_callback = [""langfuse"", ""llmonitor""] # log input/output to langfuse, llmonitor, supabase#openai callresponse = completion(model=""gpt-3.5-turbo"", messages=[{""role"": ""user"", ""content"": ""Hi 
!
 - im openai""}]) 1/26/24, 12:16â€¯PMBerriAI/litellm: Call all LLM APIs using the OpenAI format. Use Bedrâ€¦re, Anthropic, Ollama, Sagemaker, HuggingFace, Replicate (100+ LLMs)
Page 4 of 8https://github.com/BerriAI/litellm3. Cost tracking4. Rate Limiting
!
 Proxy Endpoints - Swagger DocsQuick Start Proxy - CLIStep 1: Start litellm proxyStep 2: Make ChatCompletions Request to Proxy
Proxy Key Management (Docs)Track Spend, Set budgets and create virtual keys for the proxy POST /key/generateRequestExpected Response[Beta] Proxy UIA simple UI to add new models and let your users create keys.Live here: https://dashboard.litellm.ai/pip install litellm[proxy]$ litellm --model huggingface/bigcode/starcoder#INFO: Proxy running on http://0.0.0.0:8000import openai # openai v1.0.0+client = openai.OpenAI(api_key=""anything"",base_url=""http://0.0.0.0:8000"") # set proxy to base_url# request sent to model set on litellm proxy, `litellm --model`response = client.chat.completions.create(model=""gpt-3.5-turbo"", messages = [    {        ""role"": ""user"",        ""content"": ""this is a test request, write a short poem""    }])print(response)
curl http://0.0.0.0:8000/key/generate \--header Authorization: Bearer sk-1234 \--header Content-Type: application/json \--data-raw {""models"": [""gpt-3.5-turbo"", ""gpt-4"", ""claude-2""], ""duration"": ""20m"",""metadata"": {""user"": ""ishaan@berri.ai"", ""team"": ""core-infra""}}{    ""key"": ""sk-kdEXbIqZRwEeEiHwdg7sFA"", # Bearer token    ""expires"": ""2023-11-19T01:38:25.838000+00:00"" # datetime object} 1/26/24, 12:16â€¯PMBerriAI/litellm: Call all LLM APIs using the OpenAI format. Use Bedrâ€¦re, Anthropic, Ollama, Sagemaker, HuggingFace, Replicate (100+ LLMs)
Page 5 of 8https://github.com/BerriAI/litellmCode: https://github.com/BerriAI/litellm/tree/main/ui
Supported Providers (Docs)ProviderCompletionStreamingAsyncCompletionAsyncStreamingAsyncEmbeddingAsync ImageGenerationopenai
âœ…
âœ…
âœ…
âœ…
âœ…
âœ…
azure
âœ…
âœ…
âœ…
âœ…
âœ…
âœ…
aws - sagemaker
âœ…
âœ…
âœ…
âœ…
âœ…
aws - bedrock
âœ…
âœ…
âœ…
âœ…
âœ…
google - vertex_ai[Gemini]
âœ…
âœ…
âœ…
âœ…
google - palm
âœ…
âœ…
âœ…
âœ…
google AI Studio - gemini
âœ…
âœ…
mistral ai api
âœ…
âœ…
âœ…
âœ…
âœ…
cloudflare AI Workers
âœ…
âœ…
âœ…
âœ…
cohere
âœ…
âœ…
âœ…
âœ…
âœ…
anthropic
âœ…
âœ…
âœ…
âœ…
huggingface
âœ…
âœ…
âœ…
âœ…
âœ…
replicate
âœ…
âœ…
âœ…
âœ…
together_ai
âœ…
âœ…
âœ…
âœ…
openrouter
âœ…
âœ…
âœ…
âœ…
ai21
âœ…
âœ…
âœ…
âœ… 1/26/24, 12:16â€¯PMBerriAI/litellm: Call all LLM APIs using the OpenAI format. Use Bedrâ€¦re, Anthropic, Ollama, Sagemaker, HuggingFace, Replicate (100+ LLMs)
Page 6 of 8https://github.com/BerriAI/litellmbaseten
âœ…
âœ…
âœ…
âœ…
vllm
âœ…
âœ…
âœ…
âœ…
nlp_cloud
âœ…
âœ…
âœ…
âœ…
aleph alpha
âœ…
âœ…
âœ…
âœ…
petals
âœ…
âœ…
âœ…
âœ…
ollama
âœ…
âœ…
âœ…
âœ…
deepinfra
âœ…
âœ…
âœ…
âœ…
perplexity-ai
âœ…
âœ…
âœ…
âœ…
anyscale
âœ…
âœ…
âœ…
âœ…
voyage ai
âœ…
xinference [XorbitsInference]
âœ…
Read the DocsContributingTo contribute: Clone the repo locally -> Make a change -> Submit a PR with the change.Heres how to modify the repo locally: Step 1: Clone the repoStep 2: Navigate into the project, and install dependencies:Step 3: Test your change:Step 4: Submit a PR with your changes! 
""
push your fork to your GitHub reposubmit a PR from thereSupport / talk with foundersSchedule Demo 
#
Community Discord 
$
Our numbers 
%
 +1 (770) 8783-106 / +1 (412) 618-6238Our emails 
âœ‰
 ishaan@berri.ai / krrish@berri.aiWhy did we build thisgit clone https://github.com/BerriAI/litellm.gitcd litellmpoetry installcd litellm/tests # pwd: Documents/litellm/litellm/testspoetry run flake8poetry run pytest . 1/26/24, 12:16â€¯PMBerriAI/litellm: Call all LLM APIs using the OpenAI format. Use Bedrâ€¦re, Anthropic, Ollama, Sagemaker, HuggingFace, Replicate (100+ LLMs)
Page 7 of 8https://github.com/BerriAI/litellmNeed for simplicity: Our code started to get extremely complicated managing & translating calls between Azure, OpenAI and Cohere.Contributors 1/26/24, 12:16â€¯PMBerriAI/litellm: Call all LLM APIs using the OpenAI format. Use Bedrâ€¦re, Anthropic, Ollama, Sagemaker, HuggingFace, Replicate (100+ LLMs)
Page 8 of 8https://github.com/BerriAI/litellm","{'title': 'LiteLLM', 'repository': 'BerriAI/litellm', 'description': 'A project for calling all LLM APIs using the OpenAI format, supporting various models and providers like Bedrock, Azure, OpenAI, Cohere, Anthropic, Ollama, Sagemaker, HuggingFace, and Replicate.', 'features': ['Supports 100+ LLMs including GPT-3.5-turbo, Cohere, and others.', 'Offers streaming model responses with support for all models.', 'Includes proxy server configurations for different LLM providers.'], 'usage': 'Simplifies making API calls to various LLMs and managing responses.', 'license': 'MIT', 'contributors': 'Contributions from various developers', 'repository_url': 'https://github.com/BerriAI/litellm'}"
"1/26/24, 12:16â€¯PMvanna-ai/vanna: 
ðŸ¤– Chat with your SQL database 
ðŸ“Š. Accurate Text-to-SQL Generation via LLMs using RAG 
ðŸ”„.
Page 1 of 9https://github.com/vanna-ai/vanna
vannaPublic23 Branches36 Tags
Go to file
t
Go to file
Add file
Code
Update README.mdb2c8ecfÂ Â·Â 2 days ago354 Commits.github/workflowsDelete .github/workflows/ci.â€¦last monthdocsnew notebookslast monthimgreadme updateslast monthnb-themenew notebookslast monthnotebooksupdate documentation for aâ€¦4 days agopapersdocumentation updates4 months agosrc/vannaUpdate OpenAI class for Azâ€¦4 days agotestsget related training data5 months agotraining_dataadd cybersyn-financial-dataâ€¦6 months ago.gitignoreduckdb supportlast week.pre-commit-config.yâ€¦base5 months agoCONTRIBUTING.mdupdate contributing mdlast monthLICENSEInitial commit8 months agoREADME.mdUpdate README.md2 days agopyproject.tomlUpdate pyproject.toml4 days agosetup.cfgremove unnecessary excludâ€¦5 months agotox.iniUse tox in GH action + add [â€¦5 months agoAbout
!
 Chat with your SQL database 
""
.Accurate Text-to-SQL Generation viaLLMs using RAG 
#
.vanna.ai/docs/# agent # sql # database # ai # data-visualization# text-to-sql # rag # llm Readme MIT license Activity Custom properties 4.6k stars 30 watching 256 forksReport repositoryReleases 
36v0.0.36Latest4 days ago+ 35 releasesContributors
7
Deployments
124 github-pages 3 months ago+ 123 deploymentsLanguagesJupyter Notebook92.9%Python6.8% Other0.3%GitHubPyPIDocumentationGitHubGitHubvannavannapypipypiv0.0.36v0.0.36DocumentationDocumentationvannavannaVannaVanna is an MIT-licensed open-source Python RAG (Retrieval-Augmented Generation) framework for SQL generation and relatedfunctionality.
vanna-ai/vanna
Type / to search
CodeIssues
40Pull requests
1ActionsProjects
1SecurityInsights
Â mainzainhoda
READMEMIT license 1/26/24, 12:16â€¯PMvanna-ai/vanna: 
ðŸ¤– Chat with your SQL database 
ðŸ“Š. Accurate Text-to-SQL Generation via LLMs using RAG 
ðŸ”„.
Page 2 of 9https://github.com/vanna-ai/vanna 0802.mp4 1/26/24, 12:16â€¯PMvanna-ai/vanna: 
ðŸ¤– Chat with your SQL database 
ðŸ“Š. Accurate Text-to-SQL Generation via LLMs using RAG 
ðŸ”„.
Page 3 of 9https://github.com/vanna-ai/vanna
How Vanna works 1/26/24, 12:16â€¯PMvanna-ai/vanna: 
ðŸ¤– Chat with your SQL database 
ðŸ“Š. Accurate Text-to-SQL Generation via LLMs using RAG 
ðŸ”„.
Page 4 of 9https://github.com/vanna-ai/vanna
Vanna works in two easy steps - train a RAG ""model"" on your data, and then ask questions which will return SQL queries that can be set upto automatically run on your database.1. Train a RAG ""model"" on your data.2. Ask questions. 1/26/24, 12:16â€¯PMvanna-ai/vanna: 
ðŸ¤– Chat with your SQL database 
ðŸ“Š. Accurate Text-to-SQL Generation via LLMs using RAG 
ðŸ”„.
Page 5 of 9https://github.com/vanna-ai/vanna
If you dont know what RAG is, dont worry -- you dont need to know how this works under the hood to use it. You just need to know thatyou ""train"" a model, which stores some metadata and then use it to ""ask"" questions.See the base class for more details on how this works under the hood.User InterfacesThese are some of the user interfaces that weve built using Vanna. You can use these as-is or as a starting point for your own custominterface.Jupyter Notebookvanna-ai/vanna-streamlitvanna-ai/vanna-flaskvanna-ai/vanna-slackGetting startedSee the documentation for specifics on your desired database, LLM, etc.If you want to get a feel for how it works after training, you can try this Colab notebook.InstallThere are a number of optional packages that can be installed so see the documentation for more details.ImportSee the documentation if youre customizing the LLM or vector database.pip install vanna
# The import statement will vary depending on your LLM and vector database. This is an example for OpenAI + ChromaDB 1/26/24, 12:16â€¯PMvanna-ai/vanna: 
ðŸ¤– Chat with your SQL database 
ðŸ“Š. Accurate Text-to-SQL Generation via LLMs using RAG 
ðŸ”„.
Page 6 of 9https://github.com/vanna-ai/vannaTrainingYou may or may not need to run these vn.train commands depending on your use case. See the documentation for more details.These statements are shown to give you a feel for how it works.Train with DDL StatementsDDL statements contain information about the table names, columns, data types, and relationships in your database.
Train with DocumentationSometimes you may want to add documentation about your business terminology or definitions.Train with SQLYou can also add SQL queries to your training data. This is useful if you have some queries already laying around. You can just copy andpaste those from your editor to begin generating new SQL.Asking questionsYoull get SQL# The import statement will vary depending on your LLM and vector database. This is an example for OpenAI + ChromaDBfrom vanna.openai.openai_chat import OpenAI_Chatfrom vanna.chromadb.chromadb_vector import ChromaDB_VectorStoreclass MyVanna(ChromaDB_VectorStore, OpenAI_Chat):    def __init__(self, config=None):        ChromaDB_VectorStore.__init__(self, config=config)        OpenAI_Chat.__init__(self, config=config)vn = MyVanna(config={api_key: sk-..., model: gpt-4-...})# See the documentation for other options
vn.train(ddl=""""""    CREATE TABLE IF NOT EXISTS my-table (        id INT PRIMARY KEY,        name VARCHAR(100),        age INT    )"""""")
vn.train(documentation=""Our business defines XYZ as ..."")
vn.train(sql=""SELECT name, age FROM my-table WHERE name = John Doe"")vn.ask(""What are the top 10 customers by sales?"")SELECT c.c_name as customer_name,        sum(l.l_extendedprice * (1 - l.l_discount)) as total_salesFROM   snowflake_sample_data.tpch_sf1.lineitem l join snowflake_sample_data.tpch_sf1.orders o        ON l.l_orderkey = o.o_orderkey join snowflake_sample_data.tpch_sf1.customer c        ON o.o_custkey = c.c_custkeyGROUP BY customer_nameORDER BY total_sales desc limit 10; 1/26/24, 12:16â€¯PMvanna-ai/vanna: 
ðŸ¤– Chat with your SQL database 
ðŸ“Š. Accurate Text-to-SQL Generation via LLMs using RAG 
ðŸ”„.
Page 7 of 9https://github.com/vanna-ai/vannaIf youve connected to a database, youll get the table:CUSTOMER_NAMETOTAL_SALES0Customer#0001435006757566.02181Customer#0000952576294115.33402Customer#0000871156184649.51763Customer#0001311136080943.83054Customer#0001343806075141.96355Customer#0001038346059770.32326Customer#0000696826057779.03487Customer#0001020226039653.63358Customer#0000985876027021.58559Customer#0000646605905659.6159
Youll also get an automated Plotly chart: 
ORDER BY total_sales desc limit 10; 1/26/24, 12:16â€¯PMvanna-ai/vanna: 
ðŸ¤– Chat with your SQL database 
ðŸ“Š. Accurate Text-to-SQL Generation via LLMs using RAG 
ðŸ”„.
Page 8 of 9https://github.com/vanna-ai/vannaRAG vs. Fine-TuningRAGPortable across LLMsEasy to remove training data if any of it becomes obsoleteMuch cheaper to run than fine-tuningMore future-proof -- if a better LLM comes out, you can just swap it outFine-TuningGood if you need to minimize tokens in the promptSlow to get startedExpensive to train and run (generally)Why Vanna?1. High accuracy on complex datasets.Vannaâ€™s capabilities are tied to the training data you give itMore training data means better accuracy for large and complex datasets2. Secure and private.Your database contents are never sent to the LLM or the vector databaseSQL execution happens in your local environment3. Self learning.If using via Jupyter, you can choose to ""auto-train"" it on the queries that were successfully executedIf using via other interfaces, you can have the interface prompt the user to provide feedback on the resultsCorrect question to SQL pairs are stored for future reference and make the future results more accurate4. Supports any SQL database.The package allows you to connect to any SQL database that you can otherwise connect to with Python5. Choose your front end.Most people start in a Jupyter Notebook.Expose to your end users via Slackbot, web app, Streamlit app, or a custom front end.Extending Vanna 1/26/24, 12:16â€¯PMvanna-ai/vanna: 
ðŸ¤– Chat with your SQL database 
ðŸ“Š. Accurate Text-to-SQL Generation via LLMs using RAG 
ðŸ”„.
Page 9 of 9https://github.com/vanna-ai/vanna","{'title': 'Vanna', 'repository': 'vanna-ai/vanna', 'description': 'A framework for accurate Text-to-SQL Generation via LLMs using RAG.', 'overview': {'features': ['Enables chatting with SQL databases.', 'Uses RAG for generating SQL queries from natural language inputs.', 'Aims to provide accurate SQL generation.'], 'usage': 'Can be used for various applications where SQL query generation from natural language is needed.', 'languages': ['Python'], 'license': 'MIT', 'repository_url': 'https://github.com/vanna-ai/vanna'}}"
"1/26/24, 12:15â€¯PMmckaywrigley/clarity-ai: A simple Perplexity AI clone.
Page 1 of 4https://github.com/mckaywrigley/clarity-ai
clarity-aiPublic1 Branch0 Tags
Go to file
t
Go to file
Add file
Code
Mâ€¦5a33db1Â Â·Â 10 months ago18 Commitscoâ€¦Update â€¦10 months agopaâ€¦use gpt-â€¦last yearpuâ€¦add to reâ€¦last yearstâ€¦readylast yeartyâ€¦use gpt-â€¦last yearutilsuse gpt-â€¦last year.eâ€¦readylast year.giâ€¦readylast yearRâ€¦readme â€¦last yearlicâ€¦add MIT â€¦last yearneâ€¦readylast yearpaâ€¦readylast yearpaâ€¦remove â€¦last yearpoâ€¦readylast yearAboutA simple Perplexity AI clone. Readme MIT license Activity 821 stars 23 watching 187 forksReport repositoryReleasesNo releases publishedPackagesNo packages publishedContributors
2
mckaywrigley Mckay Wrigley
Kledal Alexander KledalDeployments
17 Production 10 months ago
mâ€¦/câ€¦
Type / to search
CodeIssues
4Pull requestsActionsProjectsSecurityInsights
Â mainmckaywrigley 1/26/24, 12:15â€¯PMmckaywrigley/clarity-ai: A simple Perplexity AI clone.
Page 2 of 4https://github.com/mckaywrigley/clarity-aitaiâ€¦readylast yeartsâ€¦readylast year Preview 10 months ago+ 15 deploymentsLanguagesTypeScript97.2% JavaScript2.5%CSS0.3%Clarity AIClarity is simple perplexity.ai clone. Use the code for whatever you like! :)If you have any questions, feel free to reach out to me on Twitter.
How It WorksGiven a query, Clarity fetches relevant, up-to-date information from theweb and uses OpenAIs API to generate an answer.The app works as follows:1. Get query from user2. Scrape Google for relevant webpages3. Parse webpages for textREADMEMIT license 1/26/24, 12:15â€¯PMmckaywrigley/clarity-ai: A simple Perplexity AI clone.
Page 3 of 4https://github.com/mckaywrigley/clarity-ai3. Parse webpages for text4. Build prompt using query + webpage text5. Call OpenAI API to generate answer6. Stream answer back to userRequirementsGet OpenAI API key here.Running Locally1. Clone repo2. Install dependencies3. Run appImprovement IdeasHere are some ideas for how to improve Clarity:
Creditsgit clone https://github.com/mckaywrigley/clarity-ai.gitnpm inpm run dev
 Speed up answers by replacing link scraping with the Google SearchAPI (scraping was used to circumvent cost + rate limits)
 Add ""follow up"" searches
 Improve the prompt
 Get sources working in non text-davinci-003 models
 Train your own model to use for answer synthesis 1/26/24, 12:15â€¯PMmckaywrigley/clarity-ai: A simple Perplexity AI clone.
Page 4 of 4https://github.com/mckaywrigley/clarity-aiCreditsShoutout to Perplexity AI for the inspiration. I highly recommend checkingtheir product out.This repo is meant to show people that you can build powerful apps likePerplexity even if you dont have a large, experienced team.LLMs are amazing, and I hope Clarity inspires you to build something cool!","{'title': 'Clarity', 'repository': 'mckaywrigley/clarity', 'description': ""A simple interface designed to fetch up-to-date information from the web, using OpenAI's API to generate answers."", 'key_features': ['Fetches relevant information from the web based on user queries.', ""Uses OpenAI's API to process queries and generate accurate answers."", 'Simple and user-friendly interface.'], 'usage': 'Helps users obtain accurate and current information quickly.', 'license': 'MIT', 'repository_url': 'https://github.com/mckaywrigley/clarity'}"
"1/26/24, 12:13â€¯PMpytorch/pytorch: Tensors and Dynamic neural networks in Python with strong GPU acceleration
Page 1 of 12https://github.com/pytorch/pytorch
pytorchPublic10,181 Branches4,282 Tags
Go to file
t
Go to file
Add file
Code
andRâ€¦25f7219Â Â·Â 5 minutes ago68,767 Commits.ci[executorch hash update] uâ€¦13 hours ago.circleci[EZ][BE] Move build_androâ€¦last week.ctags.dAdd a .ctags.d/ toplevel direâ€¦5 years ago.devcontainer[Dev Container]Add readmeâ€¦4 months ago.githubFix mergeability check for gâ€¦15 hours ago.vscode[3/3] Update .pyi Python stâ€¦last yearandroidFix syntax highlighting in anâ€¦last weekatenfix lint for cudnn codes (#11â€¦19 minutes agobenchmarksMove skip sets into a new filâ€¦2 days agobinariesRemaining replacement of câ€¦4 months agoc10Get Device instance with coâ€¦2 days agocaffe2Add missing cuda libraries fâ€¦yesterdaycmake[ROCm] backward compatibâ€¦9 hours agodocs[Export] Remove ScriptObjeâ€¦17 hours agofunctorchFix missing words in READMâ€¦3 weeks agoios[BE]: Enable F821 and fix buâ€¦last monthmodules[Cmake] Check that gcc-9.â€¦2 months agomypy_pluginsEnable UFMT on a bunch of â€¦6 months agoscriptsUpdate update_failures.py gâ€¦yesterdaytestRealize inputs to DynamicScâ€¦5 minutes agothird_partyUpdate PocketFFT submodâ€¦16 hours agotools[Exception] [6/N] Remove uâ€¦2 days agotorchRealize inputs to DynamicScâ€¦5 minutes agotorchgenFix return type hint for list tyâ€¦18 hours agoAboutTensors and Dynamic neural networks inPython with strong GPU accelerationpytorch.org# python # machine-learning # deep-learning# neural-network # gpu # numpy # autograd# tensor Readme View license Code of conduct Security policy Activity Custom properties 74.8k stars 1.7k watching 20.5k forksReport repositoryReleases 
48PyTorch 2.1.2 Release, bug fiâ€¦Lateston Dec 14, 2023+ 47 releasesPackagesNo packages publishedUsed by 
370k
+ 370,416Contributors
3,117
+ 3,103 contributors
pytorch/pytorch
Type / to search
CodeIssues
5k+Pull requests
838ActionsProjects
30WikiSecurity
1Insights
Â mainezyangpytorchmergebot
 Cite this repository 1/26/24, 12:13â€¯PMpytorch/pytorch: Tensors and Dynamic neural networks in Python with strong GPU acceleration
Page 2 of 12https://github.com/pytorch/pytorch.bazelignoreput third_party/ittapi/ in .baâ€¦8 months ago.bazelrc[bazel] add python targets (â€¦8 months ago.bazelversionupdate Bazel to the latest reâ€¦10 months ago.buckconfig.oss[BE] Add regression test for â€¦10 months ago.clang-format[BE][MPS] Add MPS to clanâ€¦10 months ago.clang-tidy[BE]: Enable readability-redâ€¦last month.cmakelintrcFix/relax CMake linter rules (â€¦4 years ago.coveragercUse JIT Plug-in for coveragâ€¦3 years ago.dockerignoreAdd .dockerignore. (#3333)7 years ago.flake8Enhance torch.vmap supporâ€¦4 days ago.gdbinitgdb special command to priâ€¦3 years ago.git-blame-ignore-revsAdd 116583 to .git-blame-â€¦3 weeks ago.gitattributesthird_party: Fix build_bundlâ€¦2 years ago.gitignore[no ci] Add .watchman to .gitâ€¦2 months ago.gitmodulesRevert ""[Reland2] Update Nâ€¦last month.isort.cfgRemove pyproject.toml (#61â€¦3 years ago.lintrunner.tomlfix lint for cudnn codes (#11â€¦19 minutes ago.lldbinitAdd helpful pretty pretting sâ€¦10 months agoBUCK.oss[4] move pt_operator_librarâ€¦2 years agoBUILD.bazel[BE] [cuDNN] Always build â€¦3 weeks agoCITATION.cffuse cff standard for citation â€¦2 years agoCMakeLists.txt[2/4] Intel GPU Runtime Upsâ€¦last weekCODEOWNERS[no ci] Add pytorch-dev-infrâ€¦3 weeks agoCODE_OF_CONDUCTâ€¦Create CODE_OF_CONDUCâ€¦4 years agoCONTRIBUTING.md[DevX] Add tool and doc on â€¦last monthDockerfileDockerfile; Add cuda bin to â€¦2 weeks agoGLOSSARY.mdAdd remaining ToCs to ToC lâ€¦3 years agoLICENSE[Model Averaging] Support â€¦2 years agoMANIFEST.infix citation file in MANIFEST â€¦2 years agoMakefilemake triton uses the wheel â€¦10 months agoNOTICEAdd uint8 support for interpâ€¦last yearREADME.md[CMake] Explicitly error out â€¦18 hours agoDeployments
500+ upload-stats conda-aws-upload 2 hours ago pytorchbot-env+ more deploymentsLanguagesPython49.4% C++40.9%Cuda3.8% C2.1%Objective-C++1.4% CMake0.8%Other1.6% 1/26/24, 12:13â€¯PMpytorch/pytorch: Tensors and Dynamic neural networks in Python with strong GPU acceleration
Page 3 of 12https://github.com/pytorch/pytorchRELEASE.md[release] Add Launch Execuâ€¦3 weeks agoSECURITY.mdUpdate SECURITY.MD (#93â€¦last yearWORKSPACE[BE] [cuDNN] Always build â€¦3 weeks agoaten.bzladd explicit vectorization foâ€¦10 months agobuckbuild.bzl[pt-vulkan] Enable Python câ€¦last monthbuild.bzl[ROCm] Disabling Kernel Asâ€¦last monthbuild_variables.bzladd _amp_foreach_non_finiâ€¦last weekc2_defs.bzl[caffe2] Add option for buildâ€¦last monthc2_test_defs.bzlAdd all bzl files per D36874â€¦2 years agodefs.bzlRemove unused build systeâ€¦4 months agodocker.Makefile[releng] Docker release Refâ€¦last monthmypy-inductor.ini[mypy] Enable follow_imporâ€¦yesterdaymypy-strict.ini[pytree] Extract reusable geâ€¦3 months agomypy.iniMove test_utils.py back to â€¦2 months agopt_ops.bzl[xplat][buck2][typing] Fix tyâ€¦4 months agopt_template_srcs.bzlUse global variables to regisâ€¦4 months agopyproject.toml[BE]: Enable F821 and fix buâ€¦last monthpytest.iniReduce pytest prints (#1170â€¦3 days agorequirements-flake8.txtUpdate TorchFix to 0.2.0 (#1â€¦2 months agorequirements.txtPin the version of expecttesâ€¦last monthsetup.pyexport ATen/native/sparse/*.â€¦19 hours agoubsan.suppUpgrade Pybind submoduleâ€¦7 months agoufunc_defs.bzlmove build_variables.bzl anâ€¦2 years agoversion.txt[releng] version 2.2 -> 2.3 (â€¦last month 1/26/24, 12:13â€¯PMpytorch/pytorch: Tensors and Dynamic neural networks in Python with strong GPU acceleration
Page 4 of 12https://github.com/pytorch/pytorch
PyTorch is a Python package that provides two high-level features:Tensor computation (like NumPy) with strong GPU accelerationDeep neural networks built on a tape-based autograd systemYou can reuse your favorite Python packages such as NumPy, SciPy, and Cython to extend PyTorch when needed.Our trunk health (Continuous Integration signals) can be found at hud.pytorch.org.More About PyTorchA GPU-Ready Tensor LibraryDynamic Neural Networks: Tape-Based AutogradPython FirstImperative ExperiencesFast and LeanExtensions Without PainInstallationBinariesNVIDIA Jetson PlatformsFrom SourcePrerequisitesInstall DependenciesGet the PyTorch SourceInstall PyTorchAdjust Build Options (Optional)Docker ImageUsing pre-built imagesBuilding the image yourselfBuilding the DocumentationPrevious VersionsGetting StartedResourcesCommunicationReleases and ContributingThe TeamLicenseMore About PyT orchLearn the basics of PyTorchAt a granular level, PyTorch is a library that consists of the following components:ComponentDescriptiontorchA Tensor library like NumPy, with strong GPU supporttorch.autogradA tape-based automatic differentiation library that supports all differentiable Tensor operations in torchtorch.jitA compilation stack (TorchScript) to create serializable and optimizable models from PyTorch codetorch.nnA neural networks library deeply integrated with autograd designed for maximum flexibility 1/26/24, 12:13â€¯PMpytorch/pytorch: Tensors and Dynamic neural networks in Python with strong GPU acceleration
Page 5 of 12https://github.com/pytorch/pytorchtorch.multiprocessingPython multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for dataloading and Hogwild trainingtorch.utilsDataLoader and other utility functions for convenienceUsually, PyTorch is used either as:A replacement for NumPy to use the power of GPUs.A deep learning research platform that provides maximum flexibility and speed.Elaborating Further:A GPU-Ready Tensor LibraryIf you use NumPy, then you have used Tensors (a.k.a. ndarray).
PyTorch provides Tensors that can live either on the CPU or the GPU and accelerates the computation by a huge amount.We provide a wide variety of tensor routines to accelerate and fit your scientific computation needs such as slicing, indexing, mathematicaloperations, linear algebra, reductions. And they are fast!Dynamic Neural Networks: Tape-Based AutogradPyTorch has a unique way of building neural networks: using and replaying a tape recorder.Most frameworks such as TensorFlow, Theano, Caffe, and CNTK have a static view of the world. One has to build a neural network andreuse the same structure again and again. Changing the way the network behaves means that one has to start from scratch.With PyTorch, we use a technique called reverse-mode auto-differentiation, which allows you to change the way your network behavesarbitrarily with zero lag or overhead. Our inspiration comes from several research papers on this topic, as well as current and past work suchas torch-autograd, autograd, Chainer, etc.While this technique is not unique to PyTorch, its one of the fastest implementations of it to date. You get the best of speed and flexibilityfor your crazy research. 1/26/24, 12:13â€¯PMpytorch/pytorch: Tensors and Dynamic neural networks in Python with strong GPU acceleration
Page 6 of 12https://github.com/pytorch/pytorch
Python FirstPyTorch is not a Python binding into a monolithic C++ framework. It is built to be deeply integrated into Python. You can use it naturally likeyou would use NumPy / SciPy / scikit-learn etc. You can write your new neural network layers in Python itself, using your favorite librariesand use packages such as Cython and Numba. Our goal is to not reinvent the wheel where appropriate.Imperative ExperiencesPyTorch is designed to be intuitive, linear in thought, and easy to use. When you execute a line of code, it gets executed. There isnt anasynchronous view of the world. When you drop into a debugger or receive error messages and stack traces, understanding them isstraightforward. The stack trace points to exactly where your code was defined. We hope you never spend hours debugging your codebecause of bad stack traces or asynchronous and opaque execution engines.Fast and LeanPyTorch has minimal framework overhead. We integrate acceleration libraries such as Intel MKL and NVIDIA (cuDNN, NCCL) to maximizespeed. At the core, its CPU and GPU Tensor and neural network backends are mature and have been tested for years.Hence, PyTorch is quite fast â€” whether you run small or large neural networks.The memory usage in PyTorch is extremely efficient compared to Torch or some of the alternatives. Weve written custom memoryallocators for the GPU to make sure that your deep learning models are maximally memory efficient. This enables you to train bigger deeplearning models than before.Extensions Without PainWriting new neural network modules, or interfacing with PyTorchs Tensor API was designed to be straightforward and with minimalabstractions.You can write new neural network layers in Python using the torch API or your favorite NumPy-based libraries such as SciPy.If you want to write your layers in C/C++, we provide a convenient extension API that is efficient and with minimal boilerplate. No wrappercode needs to be written. You can see a tutorial here and an example here.InstallationBinariesCommands to install binaries via Conda or pip wheels are on our website: https://pytorch.org/get-started/locally/NVIDIA Jetson PlatformsPython wheels for NVIDIAs Jetson Nano, Jetson TX1/TX2, Jetson Xavier NX/AGX, and Jetson AGX Orin are provided here and the L4Tcontainer is published hereThey require JetPack 4.2 and above, and @dusty-nv and @ptrblck are maintaining them.From SourcePrerequisitesIf you are installing from source, you will need:Python 3.8 or later (for Linux, Python 3.8.1+ is needed) 1/26/24, 12:13â€¯PMpytorch/pytorch: Tensors and Dynamic neural networks in Python with strong GPU acceleration
Page 7 of 12https://github.com/pytorch/pytorchA compiler that fully supports C++17, such as clang or gcc (especially for aarch64, gcc 9.4.0 or newer is required)We highly recommend installing an Anaconda environment. You will get a high-quality BLAS library (MKL) and you get controlleddependency versions regardless of your Linux distro.If you want to compile with CUDA support, select a supported version of CUDA from our support matrix, then install the following:NVIDIA CUDANVIDIA cuDNN v8.5 or aboveCompiler compatible with CUDANote: You could refer to the cuDNN Support Matrix for cuDNN versions with the various supported CUDA, CUDA driver and NVIDIAhardwareIf you want to disable CUDA support, export the environment variable USE_CUDA=0. Other potentially useful environment variables may befound in setup.py.If you are building for NVIDIAs Jetson platforms (Jetson Nano, TX1, TX2, AGX Xavier), Instructions to install PyTorch for Jetson Nano areavailable hereIf you want to compile with ROCm support, installAMD ROCm 4.0 and above installationROCm is currently supported only for Linux systems.If you want to disable ROCm support, export the environment variable USE_ROCM=0. Other potentially useful environment variables may befound in setup.py.Install DependenciesCommonOn Linux
On MacOS
On Windowsconda install cmake ninja# Run this command from the PyTorch directory after cloning the source code using the â€œGet the PyTorch Sourceâ€œ section belowpip install -r requirements.txt
conda install intel::mkl-static intel::mkl-include# CUDA only: Add LAPACK support for the GPU if neededconda install -c pytorch magma-cuda110  # or the magma-cuda* that matches your CUDA version from https://anaconda.org/pytorch/repo# (optional) If using torch.compile with inductor/triton, install the matching version of triton# Run from the pytorch directory after cloningmake triton# Add this package on intel x86 processor machines onlyconda install intel::mkl-static intel::mkl-include# Add these packages if torch.distributed is neededconda install pkg-config libuvconda install intel::mkl-static intel::mkl-include# Add these packages if torch.distributed is needed.# Distributed package support on Windows is a prototype feature and is subject to changes. 1/26/24, 12:13â€¯PMpytorch/pytorch: Tensors and Dynamic neural networks in Python with strong GPU acceleration
Page 8 of 12https://github.com/pytorch/pytorchGet the PyTorch Source
Install PyTorchOn LinuxIf you would like to compile PyTorch with new C++ ABI enabled, then first run this command:If youre compiling for AMD ROCm then first run this command:Install PyTorchAside: If you are using Anaconda, you may experience an error caused by the linker:
This is caused by ld from the Conda environment shadowing the system ld. You should use a newer version of Python that fixesthis issue. The recommended Python version is 3.8.1+.On macOSOn WindowsChoose Correct Visual Studio Version.PyTorch CI uses Visual C++ BuildTools, which come with Visual Studio Enterprise, Professional, or Community Editions. You can also installthe build tools from https://visualstudio.microsoft.com/visual-cpp-build-tools/. The build tools do not come with Visual Studio Code bydefault.If you want to build legacy python code, please refer to Building on legacy code and CUDACPU-only buildsIn this mode PyTorch computations will run on your CPU, not your GPU# Distributed package support on Windows is a prototype feature and is subject to changes.conda install -c conda-forge libuv=1.39git clone --recursive https://github.com/pytorch/pytorchcd pytorch# if you are updating an existing checkoutgit submodule syncgit submodule update --init --recursive
export _GLIBCXX_USE_CXX11_ABI=1# Only run this if youre compiling for ROCmpython tools/amd_build/build_amd.pyexport CMAKE_PREFIX_PATH=${CONDA_PREFIX:-""$(dirname $(which conda))/../""}python setup.py developbuild/temp.linux-x86_64-3.7/torch/csrc/stub.o: file not recognized: file format not recognizedcollect2: error: ld returned 1 exit statuserror: command g++ failed with exit status 1
python3 setup.py develop 1/26/24, 12:13â€¯PMpytorch/pytorch: Tensors and Dynamic neural networks in Python with strong GPU acceleration
Page 9 of 12https://github.com/pytorch/pytorchNote on OpenMP: The desired OpenMP implementation is Intel OpenMP (iomp). In order to link against iomp, youll need to manuallydownload the library and set up the building environment by tweaking CMAKE_INCLUDE_PATH and LIB. The instruction here is an examplefor setting up both MKL and Intel OpenMP. Without these configurations for CMake, Microsoft Visual C OpenMP runtime (vcomp) will beused.CUDA based buildIn this mode PyTorch computations will leverage your GPU via CUDA for faster number crunchingNVTX is needed to build Pytorch with CUDA. NVTX is a part of CUDA distributive, where it is called ""Nsight Compute"". To install it onto analready installed CUDA run CUDA installation once again and check the corresponding checkbox. Make sure that CUDA with NsightCompute is installed after Visual Studio.Currently, VS 2017 / 2019, and Ninja are supported as the generator of CMake. If ninja.exe is detected in PATH, then Ninja will be usedas the default generator, otherwise, it will use VS 2017 / 2019.If Ninja is selected as the generator, the latest MSVC will get selected as the underlying toolchain.Additional libraries such as Magma, oneDNN, a.k.a. MKLDNN or DNNL, and Sccache are often needed. Please refer to the installation-helper to install them.You can refer to the build_pytorch.bat script for some other environment variables configurations
Adjust Build Options (Optional)conda activatepython setup.py develop
cmd:: Set the environment variables after you have downloaded and unzipped the mkl package,:: else CMake would throw an error as `Could NOT find OpenMP`.set CMAKE_INCLUDE_PATH={Your directory}\mkl\includeset LIB={Your directory}\mkl\lib;%LIB%:: Read the content in the previous section carefully before you proceed.:: [Optional] If you want to override the underlying toolset used by Ninja and Visual Studio with CUDA, please run the following script block.:: ""Visual Studio 2019 Developer Command Prompt"" will be run automatically.:: Make sure you have CMake >= 3.12 before you do this when you use the Visual Studio generator.set CMAKE_GENERATOR_TOOLSET_VERSION=14.27set DISTUTILS_USE_SDK=1for /f ""usebackq tokens=*"" %i in (`""%ProgramFiles(x86)%\Microsoft Visual Studio\Installer\vswhere.exe"" -version [15^,17^) -products * -latest -property installationPath`) do call ""%i\VC\Auxiliary\Build\vcvarsall.bat"" x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION% :: [Optional] If you want to override the CUDA host compilerset CUDAHOSTCXX=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.27.29110\bin\HostX64\x64\cl.exepython setup.py develop 1/26/24, 12:13â€¯PMpytorch/pytorch: Tensors and Dynamic neural networks in Python with strong GPU acceleration
Page 10 of 12https://github.com/pytorch/pytorchYou can adjust the configuration of cmake variables optionally (without building first), by doing the following. For example, adjusting thepre-detected directories for CuDNN or BLAS can be done with such a step.On LinuxOn macOSDocker ImageUsing pre-built imagesYou can also pull a pre-built docker image from Docker Hub and run with docker v19.03+Please note that PyTorch uses shared memory to share data between processes, so if torch multiprocessing is used (e.g. for multithreadeddata loaders) the default shared memory segment size that container runs with is not enough, and you should increase shared memory sizeeither with --ipc=host or --shm-size command line options to nvidia-docker run.Building the image yourselfNOTE: Must be built with a docker version > 18.06The Dockerfile is supplied to build images with CUDA 11.1 support and cuDNN v8. You can pass PYTHON_VERSION=x.y make variable tospecify which Python version is to be used by Miniconda, or leave it unset to use the default.You can also pass the CMAKE_VARS=""..."" environment variable to specify additional CMake variables to be passed to CMake during thebuild. See setup.py for the list of available variables.Building the DocumentationTo build documentation in various formats, you will need Sphinx and the readthedocs theme.You can then build the documentation by running make <format> from the docs/ folder. Run make to get a list of all available outputformats.If you get a katex error run npm install katex. If it persists, try npm install -g katexNote: if you installed nodejs with a different package manager (e.g., conda) then npm will probably install a version of katex thatis not compatible with your version of nodejs and doc builds will fail. A combination of versions that is known to work isexport CMAKE_PREFIX_PATH=${CONDA_PREFIX:-""$(dirname $(which conda))/../""}python setup.py build --cmake-onlyccmake build  # or cmake-gui buildexport CMAKE_PREFIX_PATH=${CONDA_PREFIX:-""$(dirname $(which conda))/../""}MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py build --cmake-onlyccmake build  # or cmake-gui build
docker run --gpus all --rm -ti --ipc=host pytorch/pytorch:latest
make -f docker.Makefile# images are tagged as docker.io/${your_docker_username}/pytorchCMAKE_VARS=""BUILD_CAFFE2=ON BUILD_CAFFE2_OPS=ON"" make -f docker.Makefile
cd docs/pip install -r requirements.txtREADMECode of conductLicenseSecurity 1/26/24, 12:13â€¯PMpytorch/pytorch: Tensors and Dynamic neural networks in Python with strong GPU acceleration
Page 11 of 12https://github.com/pytorch/pytorchis not compatible with your version of nodejs and doc builds will fail. A combination of versions that is known to work isnode@6.13.1 and katex@0.13.18. To install the latter with npm you can run npm install -g katex@0.13.18Previous VersionsInstallation instructions and binaries for previous PyTorch versions may be found on our website.Getting StartedThree-pointers to get you started:Tutorials: get you started with understanding and using PyTorchExamples: easy to understand PyTorch code across all domainsThe API ReferenceGlossaryResourcesPyTorch.orgPyTorch TutorialsPyTorch ExamplesPyTorch ModelsIntro to Deep Learning with PyTorch from UdacityIntro to Machine Learning with PyTorch from UdacityDeep Neural Networks with PyTorch from CourseraPyTorch TwitterPyTorch BlogPyTorch YouTubeCommunicationForums: Discuss implementations, research, etc. https://discuss.pytorch.orgGitHub Issues: Bug reports, feature requests, install issues, RFCs, thoughts, etc.Slack: The PyTorch Slack hosts a primary audience of moderate to experienced PyTorch users and developers for general chat, onlinediscussions, collaboration, etc. If you are a beginner looking for help, the primary medium is PyTorch Forums. If you need a slack invite,please fill this form: https://goo.gl/forms/PP1AGvNHpSaJP8to1Newsletter: No-noise, a one-way email newsletter with important announcements about PyTorch. You can sign-up here:https://eepurl.com/cbG0rvFacebook Page: Important announcements about PyTorch. https://www.facebook.com/pytorchFor brand guidelines, please visit our website at pytorch.orgReleases and ContributingTypically, PyTorch has three minor releases a year. Please let us know if you encounter a bug by filing an issue.We appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion.If you plan to contribute new features, utility functions, or extensions to the core, please first open an issue and discuss the feature with us.Sending a PR without discussion might end up resulting in a rejected PR because we might be taking the core in a different direction thanyou might be aware of.To learn more about making a contribution to Pytorch, please see our Contribution page. For more information about PyTorch releases, seeRelease page. 1/26/24, 12:13â€¯PMpytorch/pytorch: Tensors and Dynamic neural networks in Python with strong GPU acceleration
Page 12 of 12https://github.com/pytorch/pytorchRelease page.The T eamPyTorch is a community-driven project with several skillful engineers and researchers contributing to it.PyTorch is currently maintained by Soumith Chintala, Gregory Chanan, Dmytro Dzhulgakov, Edward Yang, and Nikita Shulga with majorcontributions coming from hundreds of talented individuals in various forms and means. A non-exhaustive but growing list needs tomention: Trevor Killeen, Sasank Chilamkurthy, Sergey Zagoruyko, Adam Lerer, Francisco Massa, Alykhan Tejani, Luca Antiga, AlbanDesmaison, Andreas Koepf, James Bradbury, Zeming Lin, Yuandong Tian, Guillaume Lample, Marat Dukhan, Natalia Gimelshein, ChristianSarofeen, Martin Raison, Edward Yang, Zachary Devito.Note: This project is unrelated to hughperkins/pytorch with the same name. Hugh is a valuable contributor to the Torch community and hashelped with many things Torch and PyTorch.","{'title': 'PyTorch', 'repository': 'pytorch/pytorch', 'description': 'A Python package for tensors and dynamic neural networks with strong GPU acceleration.', 'key_features': ['Tensor computation (like NumPy) with strong GPU acceleration.', 'Deep neural networks built on a tape-based autograd system.'], 'languages_used': ['Python', 'C++', 'Cuda', 'C', 'Objective-C++', 'CMake', 'Other'], 'license': 'Unknown', 'contributions': 'Supported by a community of engineers and researchers with major contributions from various individuals.', 'repository_url': 'https://github.com/pytorch/pytorch'}"
